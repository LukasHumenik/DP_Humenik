{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BLOK 1\n",
    "\n",
    "# importovanie potrebnych kniznic\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import json\n",
    "import pickle\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 2\n",
    "\n",
    "# nacitanie observacnych dat a cieloveho atributu\n",
    "# vyber atributu 'flux', ktory predstavuje svietivost svetelnych kriviek a ich transformacia na numpy polia\n",
    "# preskalovanie hodnot svetelnej krivky podla MinMax skalovania\n",
    "# transformacia oznacenia cieloveho atributu: over-contact - 0, semi-detached - 1, detached - 2\n",
    "\n",
    "processed = pd.read_pickle('observation_data.pkl')['processed_lightcurve']\n",
    "morphology = pd.read_pickle('observation_data.pkl')['morphology']\n",
    "curves = []\n",
    "for j in processed:\n",
    "    processed_data = eval(j)\n",
    "    processed_data = np.array(processed_data['flux'])\n",
    "    single_curve = []\n",
    "    for i in range(len(processed_data)):\n",
    "        point = []\n",
    "        point_data = (processed_data[i] - processed_data.min()) / (processed_data.max() - processed_data.min())\n",
    "        point.extend([point_data])\n",
    "        single_curve.append(point)\n",
    "    curves.append(single_curve)\n",
    "    single_curve = pd.DataFrame(single_curve)\n",
    "curves = np.array(curves)\n",
    "\n",
    "target = []\n",
    "for i in morphology:\n",
    "    if i == 'over-contact':\n",
    "        target.extend([0])\n",
    "    if i == 'semi-detached':\n",
    "        target.extend([1])\n",
    "    if i == 'detached':\n",
    "        target.extend([2])\n",
    "#target = np.array(target)\n",
    "#target = np_utils.to_categorical(target, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 3\n",
    "\n",
    "# odstranenie anomalii v observacnych datach\n",
    "# za anomalie sa povazuju svetelne krivky, ktore maju vo faze 100 odlisnu hodnotu ako vacsina svetelnych kriviek danej triedy\n",
    "# intervaly pre hodnoty svietivosti vo faze 100: over-contact <0;0.2>, semi-detached <0.35;0.75>, detached <0;0.75>\n",
    "\n",
    "\n",
    "oc = []\n",
    "for i in range(len(target)):\n",
    "    if target[i] == 0:  # over contact\n",
    "        if 0 <= curves[i][100] <= 0.2:\n",
    "            oc.append(curves[i])\n",
    "\n",
    "sd = []\n",
    "for i in range(len(target)):\n",
    "    if target[i] == 1:  # semi detached\n",
    "        if 0.35 <= curves[i][100] <= 0.75:\n",
    "            sd.append(curves[i])\n",
    "\n",
    "dt = []\n",
    "for i in range(len(target)):\n",
    "    if target[i] == 2:  # detached\n",
    "        if 0 <= curves[i][100] <= 0.75:\n",
    "            dt.append(curves[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 4\n",
    "\n",
    "# zistenie maximalnej a minimalnej hodnoty svietivosti v kazdej z 201 faz, v troch triedach osobitne\n",
    "\n",
    "oc_range = []\n",
    "for phase in range(201):\n",
    "    maximum, minimum = 0, 1\n",
    "    for curve in oc:\n",
    "        if curve[phase] > maximum:\n",
    "            maximum = curve[phase]\n",
    "        if curve[phase] < minimum:\n",
    "            minimum = curve[phase]\n",
    "    oc_range.append([minimum, maximum])\n",
    "\n",
    "sd_range = []\n",
    "for phase in range(201):\n",
    "    maximum, minimum = 0, 1\n",
    "    for curve in sd:\n",
    "        if curve[phase] > maximum:\n",
    "            maximum = curve[phase]\n",
    "        if curve[phase] < minimum:\n",
    "            minimum = curve[phase]\n",
    "    sd_range.append([minimum, maximum])\n",
    "\n",
    "dt_range = []\n",
    "for phase in range(201):\n",
    "    maximum, minimum = 0, 1\n",
    "    for curve in dt:\n",
    "        if curve[phase] > maximum:\n",
    "            maximum = curve[phase]\n",
    "        if curve[phase] < minimum:\n",
    "            minimum = curve[phase]\n",
    "    dt_range.append([minimum, maximum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 5\n",
    "\n",
    "# nacitanie syntetickych dat\n",
    "# preskalovanie hodnot syntetickej svetelnej krivky podla MinMax skalovania\n",
    "\n",
    "mor_dt = pd.read_pickle('morphology_dt.pkl')[\"lightcurve\"]\n",
    "mor_sd = pd.read_pickle('morphology_sd.pkl')[\"lightcurve\"]\n",
    "mor_oc = pd.read_pickle('morphology_oc.pkl')[\"lightcurve\"]\n",
    "data_dt = []\n",
    "for i in mor_dt:\n",
    "    data_dt.append(json.loads(i))\n",
    "data_sd = []\n",
    "for i in mor_sd:\n",
    "    data_sd.append(json.loads(i))\n",
    "data_oc = []\n",
    "for i in mor_oc:\n",
    "    data_oc.append(json.loads(i))\n",
    "\n",
    "data_dt_array = []\n",
    "for i in range(len(data_dt)):\n",
    "    data_df = pd.DataFrame(data_dt[i][0], columns=['x', 'y'])\n",
    "    data_df = (data_df['y'] - data_df['y'].min()) / (data_df['y'].max() - data_df['y'].min())\n",
    "    data_temp = []\n",
    "    for j in data_df:\n",
    "        data_temp.append([j])\n",
    "    data_dt_array.append(data_temp)\n",
    "        \n",
    "\n",
    "data_sd_array = []\n",
    "for i in range(len(data_sd)):\n",
    "    data_df = pd.DataFrame(data_sd[i][0], columns=['x', 'y'])\n",
    "    data_df = (data_df['y'] - data_df['y'].min()) / (data_df['y'].max() - data_df['y'].min())\n",
    "    data_temp = []\n",
    "    for j in data_df:\n",
    "        data_temp.append([j])\n",
    "    data_sd_array.append(data_temp)\n",
    "\n",
    "data_oc_array = []\n",
    "for i in range(len(data_oc)):\n",
    "    data_df = pd.DataFrame(data_oc[i][0], columns=['x', 'y'])\n",
    "    data_df = (data_df['y'] - data_df['y'].min()) / (data_df['y'].max() - data_df['y'].min())\n",
    "    data_temp = []\n",
    "    for j in data_df:\n",
    "        data_temp.append([j])\n",
    "    data_oc_array.append(data_temp)\n",
    "\n",
    "target_dt = []\n",
    "for i in range(len(data_dt_array)):\n",
    "    target_dt.append(2)\n",
    "\n",
    "target_sd = []\n",
    "for i in range(len(data_sd_array)):\n",
    "    target_sd.append(1)\n",
    "\n",
    "target_oc = []\n",
    "for i in range(len(data_oc_array)):\n",
    "    target_oc.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 6\n",
    "\n",
    "# vyber syntetickych kriviek na zaklade rozpÃ¤tia pre kazdu fazu, ziskaneho z observacnych kriviek\n",
    "# moznost nastavenia odchylky pre vyber kriviek pomocou premennej 'dev' - predstavuje percentualnu odchylku rozpatia v oboch smeroch\n",
    "# do premennej 'data_all' sa ukladaju krivky, do premennej 'y' sa uklada cielovy atribut\n",
    "# prevedenie 'data_all' a 'y' na numpy polia\n",
    "# rozdelenie dat na trenovaciu a testovaciu mnozinu\n",
    "# kategorizacia premennej 'y_train' a 'y_test'\n",
    "\n",
    "data_all = []\n",
    "y = []\n",
    "\n",
    "oc_syntetic = []\n",
    "for i in data_oc_array:\n",
    "    isValid = True\n",
    "    for j in range(201):\n",
    "        dev = (oc_range[j][1] - oc_range[j][0]) * 0\n",
    "        if not oc_range[j][0]-dev <= i[j] <= oc_range[j][1]+dev:\n",
    "            isValid = False\n",
    "    if isValid:\n",
    "        oc_syntetic.append(i)\n",
    "        y.append(0)\n",
    "print(\"Pocet kriviek triedy over-contact po vybere: \" + str(len(oc_syntetic)))\n",
    "\n",
    "\n",
    "sd_syntetic = []\n",
    "for i in data_sd_array:\n",
    "    isValid = True\n",
    "    for j in range(201):\n",
    "        dev = (sd_range[j][1] - sd_range[j][0]) * 0.25\n",
    "        if not sd_range[j][0]-dev <= i[j] <= sd_range[j][1]+dev:\n",
    "            isValid = False\n",
    "    if isValid:\n",
    "        sd_syntetic.append(i)\n",
    "        y.append(1)\n",
    "print(\"Pocet kriviek triedy semi-detached po vybere: \" + str(len(sd_syntetic)))\n",
    "\n",
    "\n",
    "dt_syntetic = []\n",
    "for i in data_dt_array:\n",
    "    isValid = True\n",
    "    for j in range(201):\n",
    "        dev = (dt_range[j][1] - dt_range[j][0]) * 0\n",
    "        if not dt_range[j][0]-dev <= i[j] <= dt_range[j][1]+dev:\n",
    "            isValid = False\n",
    "    if isValid:\n",
    "        dt_syntetic.append(i)\n",
    "        y.append(2)\n",
    "print(\"Pocet kriviek triedy detached po vybere: \" + str(len(dt_syntetic)))\n",
    "\n",
    "\n",
    "data_all.extend(oc_syntetic)\n",
    "data_all.extend(sd_syntetic)\n",
    "data_all.extend(dt_syntetic)\n",
    "data_all = np.array(data_all)\n",
    "print(\"Pocet vsetkych kriviek po vybere: \" + str(len(data_all)))\n",
    "print(\"Pocet vsetkych cielovych atributov po vybere: \" + str(len(y)))\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_all, y, test_size=0.2)\n",
    "y_train = np_utils.to_categorical(y_train, 3)\n",
    "y_test = np_utils.to_categorical(y_test, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 7\n",
    "\n",
    "# architektura a trenovanie MODEL 1\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Convolution1D(32, 10, activation='relu', input_shape=(201, 1)))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dense(3, activation='softmax'))\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = classifier.fit(X_train, y_train, validation_split=0.2, epochs=25, batch_size=32, verbose=1)\n",
    "accuracy = classifier.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print(\"CelkovÃ¡ presnosÅ¥ modelu 1: \" + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 8\n",
    "\n",
    "# architektura a trenovanie MODEL 2\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Convolution1D(32, 10, activation='relu', input_shape=(201, 1)))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Convolution1D(32, 10, activation='relu'))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dense(3, activation='softmax'))\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = classifier.fit(X_train, y_train, validation_split = 0.2, epochs=25, batch_size=32, verbose=1)\n",
    "accuracy = classifier.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print(\"CelkovÃ¡ presnosÅ¥ modelu 2: \" + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 9\n",
    "\n",
    "# architektura a trenovanie MODEL 3\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Convolution1D(64, 20, activation='relu', input_shape=(201, 1)))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Convolution1D(32, 10, activation='relu'))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dropout(0.25))\n",
    "classifier.add(Dense(3, activation='softmax'))\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = classifier.fit(X_train, y_train, validation_split=0.2, epochs=8, batch_size=32, verbose=1)\n",
    "accuracy = classifier.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print(\"CelkovÃ¡ presnosÅ¥ modelu 3: \" + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 10\n",
    "\n",
    "# architektura a trenovanie MODEL 4\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Convolution1D(64, 20, activation='relu', input_shape=(201, 1)))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Convolution1D(32, 10, activation='relu'))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Convolution1D(32, 10, activation='relu'))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dense(3, activation='softmax'))\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = classifier.fit(X_train, y_train, validation_split = 0.2, epochs=25, batch_size=32, verbose=1)\n",
    "accuracy = classifier.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "print(\"CelkovÃ¡ presnosÅ¥ modelu 4: \" + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 11\n",
    "\n",
    "# nacitanie ulozeneho modelu\n",
    "# MODEL1: \n",
    "# MODEL2:\n",
    "# MODEL3:\n",
    "# MODEL4:\n",
    "\n",
    "classifier = load_model('model3_filteredData.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 12\n",
    "\n",
    "# vyhodnotenie modelu na syntetickych datach pomocou kontingencnej tabulky\n",
    "# vypocet presnosti, navratnosti a f1 skore\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred2 = []\n",
    "for i in y_pred:\n",
    "    maximum = np.argmax(i)\n",
    "    y_pred2 = np.append(y_pred2, maximum)\n",
    "y_pred2 = np_utils.to_categorical(y_pred2, 3)\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_pred2.argmax(axis=1))\n",
    "print(\"Kontingencna tablulka: \" + str(cm))\n",
    "prfs = precision_recall_fscore_support(y_test.argmax(axis=1), y_pred2.argmax(axis=1), average=None)\n",
    "print(\"Presnost, navratnost, fmiera, support: \" + str(prfs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 13\n",
    "\n",
    "# vyhodnotenie modelu na observacnych datach pomocou kontingencnej tabulky\n",
    "# vypocet presnosti, navratnosti a f1 skore\n",
    "\n",
    "y_pred = classifier.predict(curves)\n",
    "y_pred2 = []\n",
    "for i in y_pred:\n",
    "    maximum = np.argmax(i)\n",
    "    y_pred2 = np.append(y_pred2, maximum)\n",
    "# plt.bar(i, height=1)\n",
    "# plt.show()\n",
    "y_pred2 = np_utils.to_categorical(y_pred2, 3)\n",
    "cm = confusion_matrix(target.argmax(axis=1), y_pred2.argmax(axis=1))\n",
    "print(\"Kontingencna tablulka: \" + str(cm))\n",
    "prfs = precision_recall_fscore_support(target.argmax(axis=1), y_pred2.argmax(axis=1), average=None)\n",
    "print(\"Presnost, navratnost, fmiera, suppert: \" + str(prfs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOK 14\n",
    "\n",
    "# vyhodnotenie zlozeneho modelu\n",
    "# prvym krokom je nacitanie modelu (classifier_all), ktorÃ½ rozhoduje, ci krivka patri do triedy over-contact, alebo nie\n",
    "# nacitanie modelu (classifier_sddt), ktory klasifikuje krivku do triedy detached,alebo semi-detached\n",
    "# vyhodnotenie na observacnych datach\n",
    "\n",
    "classifier_all = load_model(\"model3_OC-SDDT_filter.h5\")\n",
    "classifier_sddt = load_model(\"model3_SD-DT_filter.h5\")\n",
    "\n",
    "y_pred2 = []\n",
    "for i in curves:\n",
    "    prediction = classifier_all.predict(np.array([i, ]))\n",
    "    for j in prediction:\n",
    "        maximum = np.argmax(j)\n",
    "    if maximum == 0:\n",
    "        y_pred2.append(maximum)\n",
    "    else:\n",
    "        prediction = classifier_sddt.predict(np.array([i, ]))\n",
    "        for j in prediction:\n",
    "            maximum2 = np.argmax(j)\n",
    "        y_pred2.append(maximum2 + 1)\n",
    "        \n",
    "y_pred2 = np_utils.to_categorical(y_pred2, 3)\n",
    "cm = confusion_matrix(target.argmax(axis=1), y_pred2.argmax(axis=1))\n",
    "print(\"Kontingencna tablulka: \" + str(cm))\n",
    "prfs = precision_recall_fscore_support(target.argmax(axis=1), y_pred2.argmax(axis=1), average=None)\n",
    "print(\"Presnost, navratnost, fmiera, suppert: \" + str(prfs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
